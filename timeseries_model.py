# -*- coding: utf-8 -*-
"""timeseries_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZXH9e77DX2qyvzGAkpE2c3Iv4WvRoM2_

# **Import Library**
"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

from google.colab import drive
drive.mount('/content/drive')

"""# **Read Data**"""

data_train = pd.read_csv("/content/drive/Othercomputers/My Laptop/timeseries_submission/covid_19_indonesia_time_series_all.csv")
data_train

"""# **Preprocessing**"""

data_train = data_train[['Date', 'New Cases']]
data_train

scaler = MinMaxScaler()
data_train['New Cases'] = scaler.fit_transform(data_train['New Cases'].values.reshape(-1,1))

dates = data_train['Date'].values
cases  = data_train['New Cases']

plt.figure(figsize=(15,5))
plt.plot(dates, cases)
plt.title('New Cases', fontsize=20);

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_size = int(len(cases) * 0.8)
time = np.array(range(len(cases)))
series = np.array(cases)
time_train = time[:train_size]
x_train = series[:train_size]
time_valid = time[train_size:]
x_valid = series[train_size:]

train_set = windowed_dataset(
    x_train, window_size=60,
    batch_size=100,
    shuffle_buffer=1000)

valid_set = windowed_dataset(
    x_valid, window_size=60,
    batch_size=100,
    shuffle_buffer=1000)

for x, y in train_set.take(1):
    print(x[0])
    print(y[0])

"""# **Data Training**"""

model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(60, return_sequences=True),
    tf.keras.layers.LSTM(60),
    tf.keras.layers.Dense(30, activation="relu"),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation="relu"),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1),
])

threshold_mae = (data_train['New Cases'].max() - data_train['New Cases'].min()) * 10/100

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('mae')<threshold_mae):
            print("mae < 10%")
            self.model.stop_training = True

callbacks = myCallback()

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae", "accuracy"])

history = model.fit(train_set, epochs=20,
                    validation_data=valid_set,
                    callbacks=[callbacks],
                    batch_size=128)

"""# **Plot**

# **MAE**
"""

plt.plot(history.history['mae'])
plt.title('MAE')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['mae'], loc='lower right')
plt.show()

plt.plot(history.history['val_mae'])
plt.title('Validation MAE')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['MAE'], loc='lower right')
plt.show()

"""## **Accuracy**"""

plt.plot(history.history['accuracy'])
plt.title('Train Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Accuracy'], loc='lower right')
plt.show()

plt.plot(history.history['val_accuracy'])
plt.title('Validation Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Validation'], loc='lower right')
plt.show()

"""## **Loss**"""

plt.plot(history.history['loss'])
plt.title('Train Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Loss'], loc='lower right')
plt.show()

plt.plot(history.history['val_loss'])
plt.title('Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Loss'], loc='lower right')
plt.show()